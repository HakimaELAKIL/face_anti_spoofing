{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://192.168.1.35:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# 192.168.1.35\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from flask import Flask, Response\n",
    "\n",
    "# Initialisation de Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# URL du flux ESP32-CAM\n",
    "ESP32_STREAM_URL = \"http://192.168.1.35:81/stream\"  # Remplacez XX par l'IP de l'ESP32-CAM\n",
    "\n",
    "# Initialisation de Mediapipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "# Fonction de calcul pour détecter les clignements\n",
    "def calculate_blink_ratio(landmarks, left_eye_indices, right_eye_indices):\n",
    "    def get_distance(p1, p2):\n",
    "        return ((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2) ** 0.5\n",
    "\n",
    "    # Calculer les distances pour les deux yeux\n",
    "    left_eye_top = landmarks[left_eye_indices[1]]\n",
    "    left_eye_bottom = landmarks[left_eye_indices[5]]\n",
    "    left_eye_left = landmarks[left_eye_indices[0]]\n",
    "    left_eye_right = landmarks[left_eye_indices[3]]\n",
    "\n",
    "    right_eye_top = landmarks[right_eye_indices[1]]\n",
    "    right_eye_bottom = landmarks[right_eye_indices[5]]\n",
    "    right_eye_left = landmarks[right_eye_indices[0]]\n",
    "    right_eye_right = landmarks[right_eye_indices[3]]\n",
    "\n",
    "    left_vertical = get_distance(left_eye_top, left_eye_bottom)\n",
    "    left_horizontal = get_distance(left_eye_left, left_eye_right)\n",
    "\n",
    "    right_vertical = get_distance(right_eye_top, right_eye_bottom)\n",
    "    right_horizontal = get_distance(right_eye_left, right_eye_right)\n",
    "\n",
    "    left_ratio = left_vertical / left_horizontal\n",
    "    right_ratio = right_vertical / right_horizontal\n",
    "\n",
    "    return (left_ratio + right_ratio) / 2\n",
    "\n",
    "# Générer les frames pour le flux vidéo\n",
    "def generate_frames():\n",
    "    cap = cv2.VideoCapture(ESP32_STREAM_URL)\n",
    "    left_eye_indices = [33, 160, 158, 133, 153, 144]  # Indices des points de l'œil gauche\n",
    "    right_eye_indices = [362, 385, 387, 263, 373, 380]  # Indices des points de l'œil droit\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Conversion en RGB pour Mediapipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # Calculer le ratio de clignement\n",
    "                blink_ratio = calculate_blink_ratio(\n",
    "                    face_landmarks.landmark, left_eye_indices, right_eye_indices\n",
    "                )\n",
    "\n",
    "                # Vérification du clignement\n",
    "                blink_status = \"Eyes Closed\" if blink_ratio < 0.2 else \"Eyes Open\"\n",
    "\n",
    "                # Ajouter du texte à l'image\n",
    "                cv2.putText(frame, blink_status, (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        _, buffer = cv2.imencode('.jpg', frame)\n",
    "        frame = buffer.tobytes()\n",
    "\n",
    "        yield (b'--frame\\r\\n'\n",
    "               b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n",
    "\n",
    "# Route pour le flux vidéo\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "# Lancer le serveur Flask\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='192.168.1.35', port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# Création de l'application Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Chemin pour enregistrer le modèle combiné\n",
    "MODEL_PATH = \"models/svm_face_recognition_with_pca.pkl\"  # Chemin mis à jour\n",
    "ANTI_SPOOF_MODEL_PATH = \"models/anti_spoof_model.pkl\"\n",
    "\n",
    "# Fonction pour charger les modèles\n",
    "def load_models():\n",
    "    with open(MODEL_PATH, 'rb') as f:\n",
    "        svm_model, pca, label_encoder = pickle.load(f)\n",
    "    with open(ANTI_SPOOF_MODEL_PATH, 'rb') as f:\n",
    "        anti_spoof_model = pickle.load(f)\n",
    "    return svm_model, pca, label_encoder, anti_spoof_model\n",
    "\n",
    "# Fonction pour détecter les attaques anti-spoofing\n",
    "def detect_spoofing(image, anti_spoof_model):\n",
    "    image_resized = cv2.resize(image, (64, 64))  # Adapter selon le modèle\n",
    "    image_preprocessed = image_resized / 255.0\n",
    "    image_preprocessed = np.expand_dims(image_preprocessed, axis=0)  # Ajouter une dimension batch\n",
    "    prediction = anti_spoof_model.predict(image_preprocessed)\n",
    "    return prediction[0] > 0.5  # Seuil à ajuster selon votre modèle\n",
    "\n",
    "# Fonction pour prétraiter l'image\n",
    "def preprocess_image(image):\n",
    "    # Conversion en niveaux de gris (optionnel, selon vos besoins)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Normalisation des pixels\n",
    "    normalized_image = gray_image / 255.0\n",
    "\n",
    "    # Redimensionner l'image à une taille fixe pour correspondre au modèle\n",
    "    resized_image = cv2.resize(normalized_image, (160, 160))\n",
    "\n",
    "    # Flatten pour adapter aux modèles SVM (si non basé sur des convolutions)\n",
    "    flattened_image = resized_image.flatten()\n",
    "\n",
    "    return flattened_image\n",
    "\n",
    "# Fonction pour prédire l'identité avec SVM et PCA\n",
    "def recognize_face_with_svm(face_embedding, svm_model, pca, label_encoder):\n",
    "    reduced_embedding = pca.transform([face_embedding])\n",
    "    predicted_label_index = svm_model.predict(reduced_embedding)[0]\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_label_index])[0]\n",
    "    probability = max(svm_model.predict_proba(reduced_embedding)[0])\n",
    "    return predicted_label, probability\n",
    "\n",
    "# Fonction de traitement des images envoyées\n",
    "@app.route('/upload_frames', methods=['POST'])\n",
    "def upload_frames():\n",
    "    try:\n",
    "        # Charger les modèles\n",
    "        svm_model, pca, label_encoder, anti_spoof_model = load_models()\n",
    "\n",
    "        # Récupération de l'image envoyée\n",
    "        file = request.files['image']\n",
    "        user_identity = request.form.get('identity')\n",
    "        if not file:\n",
    "            return jsonify({\"error\": \"Aucune image n'a été envoyée.\"}), 400\n",
    "\n",
    "        # Lecture de l'image\n",
    "        img = cv2.imdecode(np.frombuffer(file.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "        # Détection anti-spoofing\n",
    "        if not detect_spoofing(img, anti_spoof_model):\n",
    "            return jsonify({\"error\": \"Détection de spoofing : image non valide.\"}), 403\n",
    "\n",
    "        # Prétraitement de l'image\n",
    "        face_embedding = preprocess_image(img)  # Application du prétraitement défini\n",
    "\n",
    "        # Simuler un embedding (Remplacez cette ligne par l'appel à FaceNet)\n",
    "        face_embedding = np.random.rand(128)  # Remplacez par la véritable extraction\n",
    "\n",
    "        # Reconnaissance faciale\n",
    "        label, probability = recognize_face_with_svm(face_embedding, svm_model, pca, label_encoder)\n",
    "\n",
    "        # Vérification d'identité\n",
    "        if user_identity and user_identity != label:\n",
    "            return jsonify({\"error\": \"Identité non vérifiée. Correspondance échouée.\"}), 401\n",
    "\n",
    "        # Retour des résultats\n",
    "        return jsonify({\"label\": label, \"probability\": probability})\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisateurs supprimés avec succès.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Connexion à la base de données SQLite (ajustez le chemin si nécessaire)\n",
    "conn = sqlite3.connect('face_recognition.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Obtenez la date d'aujourd'hui au format 'YYYY-MM-DD'\n",
    "aujourdhui = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Suppression des utilisateurs enregistrés avant aujourd'hui\n",
    "query = \"DELETE FROM user_data WHERE timestamp < ?\"\n",
    "cursor.execute(query, (aujourdhui,))\n",
    "\n",
    "# Validation de la transaction\n",
    "conn.commit()\n",
    "\n",
    "# Fermeture de la connexion\n",
    "conn.close()\n",
    "\n",
    "print(\"Utilisateurs supprimés avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not open video stream from ESP32.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "def start_video_stream():\n",
    "    # Replace this with the correct ESP32 stream URL\n",
    "    ESP32_STREAM_URL = \"http://192.168.137.83/capture\"\n",
    "\n",
    "    # Open video stream\n",
    "    cap = cv2.VideoCapture(ESP32_STREAM_URL)\n",
    "\n",
    "    # Check if the stream was successfully opened\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video stream from ESP32.\")\n",
    "        return\n",
    "\n",
    "    # Process video stream\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to read frame from ESP32.\")\n",
    "            break\n",
    "\n",
    "        # Display the video feed\n",
    "        cv2.imshow('ESP32 Video Stream', frame)\n",
    "\n",
    "        # Exit on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "start_video_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"messages\":[{\"messageId\":\"4345599663664335296466\",\"status\":{\"description\":\"Message sent to next instance\",\"groupId\":1,\"groupName\":\"PENDING\",\"id\":26,\"name\":\"PENDING_ACCEPTED\"},\"to\":\"212688552708\"}]}\n"
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "import json\n",
    "\n",
    "conn = http.client.HTTPSConnection(\"d9ydxr.api.infobip.com\")\n",
    "payload = json.dumps({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"destinations\": [{\"to\":\"212688552708\"}],\n",
    "            \"from\": \"ServiceSMS\",\n",
    "            \"text\": \"Congratulations on sending your first message. Go ahead and check the delivery report in the next step.\"\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "headers = {\n",
    "    'Authorization': 'App afe5bc66d5b3be2cc4c2c52cd0d395c4-893a5145-1842-4c3a-be84-ff70d3ebfb42',\n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "conn.request(\"POST\", \"/sms/2/text/advanced\", payload, headers)\n",
    "res = conn.getresponse()\n",
    "data = res.read()\n",
    "print(data.decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
